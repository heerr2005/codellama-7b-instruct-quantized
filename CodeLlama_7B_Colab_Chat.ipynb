{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ** ðŸª„ CodeLlamaâ€‘7B Instruct â€” Professional Google Colab Notebook**\n",
        "\n",
        "This notebook demonstrates how to run CodeLlamaâ€‘7Bâ€‘Instruct efficiently on Google Colab using 8â€‘bit quantization.\n",
        "It is structured for clarity, stability.\n",
        "\n",
        "# **Notebook Overview**\n",
        "\n",
        "Model: codellama/CodeLlama-7b-Instruct-hf\n",
        "Framework: Hugging Face Transformers\n",
        "Environment: Google Colab (T4 / L4 GPU)\n",
        "Quantization: 8â€‘bit (via bitsandbytes)\n",
        "\n",
        "**Why NOT pipeline()?**\n",
        "\n",
        "The Hugging Face pipeline() API loads the fullâ€‘precision model, which exceeds Colab GPU memory and causes crashes. This notebook uses manual model loading with 8â€‘bit quantization, which is the recommended and productionâ€‘safe approach."
      ],
      "metadata": {
        "id": "htOvWjswP3fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf\n"
      ],
      "metadata": {
        "id": "UeCZo2YZ7asx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 â€” Install Required Libraries\n",
        "\n",
        "**Purpose:**  \n",
        "Installs the Python dependencies needed for model loading and inference:\n",
        "- `transformers` â†’ Model & tokenizer\n",
        "- `accelerate` â†’ Device mapping\n",
        "- `bitsandbytes` â†’ 8-bit quantization\n",
        "\n",
        "ðŸ“Œ Run this cell only once at the start.\n"
      ],
      "metadata": {
        "id": "s_2_9-bIQTX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "bEjM6oDW9XSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 â€” Import Required Python Modules\n",
        "\n",
        "**Purpose:**  \n",
        "Import the core libraries used in this notebook for:\n",
        "- Model loading\n",
        "- Tokenization\n",
        "- GPU support\n",
        "\n",
        "These must be imported before any model code runs.\n"
      ],
      "metadata": {
        "id": "73EMpBfLQrRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 â€” Load CodeLlama-7B-Instruct (8-bit)\n",
        "\n",
        "**Purpose:**  \n",
        "Loads the CodeLlama-7B-Instruct model in **8-bit quantized format** so it\n",
        "can run without crashing on Google Colab GPUs.\n",
        "\n",
        "This approach:\n",
        "âœ” Reduces GPU memory usage  \n",
        "âœ” Avoids OOM crashes  \n",
        "âœ” Works reliably on T4 / L4 GPUs\n",
        "\n",
        "**Notes:**\n",
        "- The tokenizer is also loaded here\n",
        "- pad_token is fixed to prevent warnings\n"
      ],
      "metadata": {
        "id": "bRM8my2nRR2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "3sqkxpJc9eEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 â€” Sanity Test (Single Prompt)\n",
        "\n",
        "**Purpose:**  \n",
        "Verify that the model loaded correctly by giving it a simple prompt\n",
        "and printing the response. This helps ensure everything is working\n",
        "before starting the interactive chat.\n",
        "\n",
        "We pass a short message and generate a short reply.\n"
      ],
      "metadata": {
        "id": "3gm2RU67RkKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "inputs = inputs.to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(\n",
        "    outputs[0][inputs.shape[-1]:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "sAgfDwBq-v9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "mtfLUT7J_cg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 â€” Prepare Model Inputs Using Chat Template\n",
        "\n",
        "**Purpose:**\n",
        "Formats the conversation history into a model-compatible prompt\n",
        "using CodeLlamaâ€™s official chat template.\n",
        "\n",
        "This step:\n",
        "- Preserves conversation roles\n",
        "- Ensures correct assistant generation\n",
        "- Moves tensors to the appropriate device (GPU/CPU)\n"
      ],
      "metadata": {
        "id": "e1QR_-HtS12d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "inputs = inputs.to(model.device)\n"
      ],
      "metadata": {
        "id": "DDIIwrHP_lwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 â€” Define Interactive Chat Function\n",
        "\n",
        "**Purpose:**  \n",
        "This block defines a function named `chat()` that:\n",
        "âœ” Accepts user input\n",
        "âœ” Keeps a running message history\n",
        "âœ” Formats input for the model\n",
        "âœ” Generates responses\n",
        "âœ” Prints replies in chat format\n",
        "\n",
        "Youâ€™ll run this function in the next step.\n"
      ],
      "metadata": {
        "id": "N9CruWa-RyPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    messages = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(\n",
        "            outputs[0][inputs.shape[-1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        print(\"CodeLlama:\", response)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tvwE8vMD_rMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 â€” Start Interactive Chat\n",
        "\n",
        "**Purpose:**  \n",
        "Boots up the interactive chat loop so you can talk to CodeLlama.\n",
        "After running this cell:\n",
        "- A prompt will appear below\n",
        "- Type your message\n",
        "- Press Enter to get a reply\n",
        "- Type `exit` or `quit` when done\n"
      ],
      "metadata": {
        "id": "eiL_JCgbR5Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat()\n"
      ],
      "metadata": {
        "id": "T2PB-hT8BD8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… Notebook Completed\n",
        "\n",
        "Congratulations! You have:\n",
        "âœ” Installed libraries  \n",
        "âœ” Loaded a large language model in 8-bit  \n",
        "âœ” Verified model output  \n",
        "âœ” Built an interactive chat loop  \n",
        "âœ” Created a professional, GitHub-ready notebook  \n",
        "\n",
        "ðŸš€ Next steps (optional):\n",
        "- Add a web UI (Gradio)\n",
        "- Save chat logs\n",
        "- Add more system prompts\n"
      ],
      "metadata": {
        "id": "6zis6zQmSBUa"
      }
    }
  ]
}